{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt1PHfLU9Hk6s46prkZGut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V1k1ng04/Federated-Learning/blob/main/ResNeXt-50_unstable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flwr[simulation] flwr_datasets[vision] torch torchvision matplotlib"
      ],
      "metadata": {
        "id": "uPWsK8Rbzu_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d13653-1aae-4926-b107-5b30c1007d06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HKGeRghqm01G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a49d92-0ada-4eb5-b50a-f69c3990f36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu using PyTorch 2.3.0+cu121 and Flower 1.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: validation loss 0.285870982170105, accuracy 0.409\n",
            "Epoch 2: validation loss 0.4266102156639099, accuracy 0.39\n",
            "Epoch 3: validation loss 0.055126660943031314, accuracy 0.467\n",
            "Epoch 4: validation loss 0.055635445892810824, accuracy 0.532\n",
            "Epoch 5: validation loss 0.09619136202335357, accuracy 0.495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
            "INFO:flwr:Starting Flower simulation, config: num_rounds=5, no round_timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test set performance:\n",
            "\tloss 0.1028998011469841\n",
            "\taccuracy 0.5074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-06-01 04:07:21,778 E 4237 4237] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 9 failures\n",
            "INFO:flwr:aggregate_fit: received 1 results and 9 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 4 failures\n",
            "INFO:flwr:aggregate_evaluate: received 1 results and 4 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "INFO:flwr:[ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 9 failures\n",
            "INFO:flwr:aggregate_fit: received 1 results and 9 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 4 failures\n",
            "INFO:flwr:aggregate_evaluate: received 1 results and 4 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "INFO:flwr:[ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 9 failures\n",
            "INFO:flwr:aggregate_fit: received 1 results and 9 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 4 failures\n",
            "INFO:flwr:aggregate_evaluate: received 1 results and 4 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "INFO:flwr:[ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 9 failures\n",
            "INFO:flwr:aggregate_fit: received 1 results and 9 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 4 failures\n",
            "INFO:flwr:aggregate_evaluate: received 1 results and 4 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "INFO:flwr:[ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 10)\n",
            "INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 9 failures\n",
            "INFO:flwr:aggregate_fit: received 1 results and 9 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 167fe63a826af3d50570b0c9f709b831ea8d1ee4b7100efe5c7bbbb0) where the task (actor ID: 0aa90ed13744d673caae75f701000000, name=ClientAppActor.__init__, pid=4355, memory used=3.02GB) was running was 12.04GB / 12.67GB (0.950224), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-cc261adb68196c26dbb33067bb033d39cb6540a7aadcdc7583d16287*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "377\t3.82\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-85079d28-807f...\n",
            "4356\t3.65\tray::ClientAppActor.run\n",
            "4355\t3.02\tray::ClientAppActor.run\n",
            "920\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:431aa03f337151986...\n",
            "81\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4263\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "4208\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "4203\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "4265\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --logs-dir=/...\n",
            "59\t0.03\tpython3 /usr/local/bin/colab-fileshim.py\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[2m\u001b[36m(ClientAppActor pid=4356)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 4 failures\n",
            "INFO:flwr:aggregate_evaluate: received 1 results and 4 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "INFO:flwr:[SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 5 rounds in 842.60s\n",
            "INFO:flwr:Run finished 5 rounds in 842.60s\n",
            "\u001b[92mINFO \u001b[0m:      History (loss, distributed):\n",
            "INFO:flwr:History (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t('\\tround 1: 0.04529278469085694\\n'\n",
            "INFO:flwr:\t('\\tround 1: 0.04529278469085694\\n'\n",
            "\u001b[92mINFO \u001b[0m:      \t '\\tround 2: 0.1471500414609909\\n'\n",
            "INFO:flwr:\t '\\tround 2: 0.1471500414609909\\n'\n",
            "\u001b[92mINFO \u001b[0m:      \t '\\tround 3: 0.044728513777256014\\n'\n",
            "INFO:flwr:\t '\\tround 3: 0.044728513777256014\\n'\n",
            "\u001b[92mINFO \u001b[0m:      \t '\\tround 4: 0.11923786520957946\\n'\n",
            "INFO:flwr:\t '\\tround 4: 0.11923786520957946\\n'\n",
            "\u001b[92mINFO \u001b[0m:      \t '\\tround 5: 0.04340324532985687\\n')History (metrics, distributed, evaluate):\n",
            "INFO:flwr:\t '\\tround 5: 0.04340324532985687\\n')History (metrics, distributed, evaluate):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.505), (2, 0.453), (3, 0.535), (4, 0.417), (5, 0.576)]}\n",
            "INFO:flwr:\t{'accuracy': [(1, 0.505), (2, 0.453), (3, 0.535), (4, 0.417), (5, 0.576)]}\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "INFO:flwr:\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from datasets.utils.logging import disable_progress_bar\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnext50_32x4d  # Import EfficientNet\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr_datasets import FederatedDataset\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")\n",
        "disable_progress_bar()\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_datasets():\n",
        "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": NUM_CLIENTS})\n",
        "\n",
        "    def apply_transforms(batch):\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ]\n",
        "        )\n",
        "        batch[\"img\"] = [transform(img) for img in batch[\"img\"]]\n",
        "        return batch\n",
        "\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for partition_id in range(NUM_CLIENTS):\n",
        "        partition = fds.load_partition(partition_id, \"train\")\n",
        "        partition = partition.with_transform(apply_transforms)\n",
        "        partition = partition.train_test_split(train_size=0.8, seed=42)\n",
        "        trainloaders.append(DataLoader(partition[\"train\"], batch_size=BATCH_SIZE))\n",
        "        valloaders.append(DataLoader(partition[\"test\"], batch_size=BATCH_SIZE))\n",
        "    testset = fds.load_split(\"test\").with_transform(apply_transforms)\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets()\n",
        "\n",
        "def train(net, trainloader, epochs: int, verbose=False):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for batch in trainloader:\n",
        "            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "def test(net, testloader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n",
        "\n",
        "trainloader = trainloaders[0]\n",
        "valloader = valloaders[0]\n",
        "net = resnext50_32x4d(pretrained=True)\n",
        "net.fc = nn.Linear(net.fc.in_features, 10)  # Adjust for CIFAR-10\n",
        "net = net.to(DEVICE)\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(net, trainloader, 1)\n",
        "    loss, accuracy = test(net, valloader)\n",
        "    print(f\"Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}\")\n",
        "\n",
        "loss, accuracy = test(net, testloader)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")\n",
        "\n",
        "def set_parameters(net, parameters: List[np.ndarray]):\n",
        "    params_dict = zip(net.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "def get_parameters(net) -> List[np.ndarray]:\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net, trainloader, valloader):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return get_parameters(self.net)\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        return get_parameters(self.net), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        set_parameters(self.net, parameters)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "def client_fn(cid: str) -> FlowerClient:\n",
        "    net = resnext50_32x4d(pretrained=True)\n",
        "    net.fc = nn.Linear(net.fc.in_features, 10)  # Adjust for CIFAR-10\n",
        "    net = net.to(DEVICE)\n",
        "    trainloader = trainloaders[int(cid)]\n",
        "    valloader = valloaders[int(cid)]\n",
        "    return FlowerClient(net, trainloader, valloader)\n",
        "\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=0.5,\n",
        "    min_fit_clients=10,\n",
        "    min_evaluate_clients=5,\n",
        "    min_available_clients=10,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "client_resources = {\"num_cpus\": 1, \"num_gpus\": 0.0}\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_cpus\": 1, \"num_gpus\": 1.0}\n",
        "\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=5),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_history = history.metrics_distributed[\"accuracy\"]\n",
        "rounds = [entry[0] for entry in accuracy_history]\n",
        "accuracies = [entry[1] * 100 for entry in accuracy_history]\n",
        "\n",
        "plt.plot(rounds, accuracies, marker='o')\n",
        "plt.grid()\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.title(\"CIFAR-10 - ResNext-50 Model Accuracy Over 5 Rounds for 10 clients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nzn_nv-CztM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "421244ce-fa5a-4992-941c-b9fb03205719"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}